@article{Dean2008MapReduce,
author = {Dean, Jeffrey and Ghemawat, Sanjay},
title = {MapReduce: Simplified Data Processing on Large Clusters},
year = {2008},
issue_date = {January 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/1327452.1327492},
doi = {10.1145/1327452.1327492},
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
journal = {Commun. ACM},
month = {jan},
pages = {107â€“113},
numpages = {7}
}

@article{Gandomi2015Beyond,
title = {Beyond the hype: Big data concepts, methods, and analytics},
journal = {International Journal of Information Management},
volume = {35},
number = {2},
pages = {137-144},
year = {2015},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2014.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0268401214001066},
author = {Amir Gandomi and Murtaza Haider},
keywords = {Big data analytics, Big data definition, Unstructured data analytics, Predictive analytics},
abstract = {Size is the first, and at times, the only dimension that leaps out at the mention of big data. This paper attempts to offer a broader definition of big data that captures its other unique and defining characteristics. The rapid evolution and adoption of big data by industry has leapfrogged the discourse to popular outlets, forcing the academic press to catch up. Academic journals in numerous disciplines, which will benefit from a relevant discussion of big data, have yet to cover the topic. This paper presents a consolidated description of big data by integrating definitions from practitioners and academics. The paper's primary focus is on the analytic methods used for big data. A particular distinguishing feature of this paper is its focus on analytics related to unstructured data, which constitute 95% of big data. This paper highlights the need to develop appropriate and efficient analytical methods to leverage massive volumes of heterogeneous data in unstructured text, audio, and video formats. This paper also reinforces the need to devise new tools for predictive analytics for structured big data. The statistical methods in practice were devised to infer from sample data. The heterogeneity, noise, and the massive size of structured big data calls for developing computationally efficient algorithms that may avoid big data pitfalls, such as spurious correlation.}
}

@inproceedings{Beaver2010Finding,
  title={Finding a Needle in Haystack: Facebook's Photo Storage.},
  author={Beaver, Doug and Kumar, Sanjeev and Li, Harry C and Sobel, Jason and Vajgel, Peter and others},
  booktitle={OSDI},
  volume={10},
  number={2010},
  pages={1--8},
  year={2010}
}
